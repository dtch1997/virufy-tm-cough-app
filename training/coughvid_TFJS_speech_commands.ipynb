{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coughvid_TFJS_speech_commands.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CPD_sIrum8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2594779-f9da-4e81-ff15-7cd3979249e5"
      },
      "source": [
        "!pip install librosa tensorflowjs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/1f/632d04bec71d4736a4e0e512cf41236b3416ac00d0a532f6511a829d18c9/tensorflowjs-3.3.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Collecting tensorflow-hub<0.10,>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.4.1)\n",
            "Requirement already satisfied: h5py<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (54.0.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa) (1.14.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub<0.10,>=0.7.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.32.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (1.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (3.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Installing collected packages: tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: tensorflow-hub 0.11.0\n",
            "    Uninstalling tensorflow-hub-0.11.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.11.0\n",
            "Successfully installed tensorflow-hub-0.9.0 tensorflowjs-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLtrccz0uxj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77777aa1-39b2-43fc-d8fc-d573bff7ccb5"
      },
      "source": [
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "import tqdm\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tfjs.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BhMZ5myu1lG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913298d2-9291-45eb-a7aa-6444d41db2be"
      },
      "source": [
        "# Download the TensorFlow.js Speech Commands model and the associated\n",
        "# preprocesssing model.\n",
        "!mkdir -p /tmp/tfjs-sc-model\n",
        "!curl -o /tmp/tfjs-sc-model/metadata.json -fsSL https://storage.googleapis.com/tfjs-models/tfjs/speech-commands/v0.3/browser_fft/18w/metadata.json\n",
        "!curl -o /tmp/tfjs-sc-model/model.json -fsSL https://storage.googleapis.com/tfjs-models/tfjs/speech-commands/v0.3/browser_fft/18w/model.json\n",
        "!curl -o /tmp/tfjs-sc-model/group1-shard1of2 -fSsL https://storage.googleapis.com/tfjs-models/tfjs/speech-commands/v0.3/browser_fft/18w/group1-shard1of2\n",
        "!curl -o /tmp/tfjs-sc-model/group1-shard2of2 -fsSL https://storage.googleapis.com/tfjs-models/tfjs/speech-commands/v0.3/browser_fft/18w/group1-shard2of2\n",
        "!curl -o /tmp/tfjs-sc-model/sc_preproc_model.tar.gz -fSsL https://storage.googleapis.com/tfjs-models/tfjs/speech-commands/conversion/sc_preproc_model.tar.gz\n",
        "!cd /tmp/tfjs-sc-model/ && tar xzvf sc_preproc_model.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sc_preproc_model/\n",
            "sc_preproc_model/assets/\n",
            "sc_preproc_model/variables/\n",
            "sc_preproc_model/variables/variables.data-00000-of-00001\n",
            "sc_preproc_model/variables/variables.index\n",
            "sc_preproc_model/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8B-Vof8u3Pj"
      },
      "source": [
        "# Download Speech Commands v0.02 dataset. The dataset contains 30+ word and\n",
        "# sound categories, but we will only use a subset of them\n",
        "\n",
        "!mkdir -p /tmp/speech_commands_v0.02\n",
        "!curl -o /tmp/speech_commands_v0.02/speech_commands_v0.02.tar.gz -fSsL http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "!cd  /tmp/speech_commands_v0.02 && tar xzf speech_commands_v0.02.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYjaIWGEu4vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30f03cb-272e-4bb9-b47d-a07427041456"
      },
      "source": [
        "# Load the preprocessing model, which transforms audio waveform into \n",
        "# spectrograms (2D image-like representation of sound).\n",
        "# This preprocessing model replicates WebAudio's AnalyzerNode.getFloatFrequencyData\n",
        "# (https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData).\n",
        "# It performs short-time Fourier transform (STFT) using a length-2048 Blackman\n",
        "# window. It opeartes on mono audio at the 44100-Hz sample rate.\n",
        "\n",
        "preproc_model_path = '/tmp/tfjs-sc-model/sc_preproc_model'\n",
        "preproc_model = tf.keras.models.load_model(preproc_model_path)\n",
        "preproc_model.summary()\n",
        "preproc_model.input_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"audio_preproc\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "audio_preprocessing_layer (A (None, None, 232, 1)      2048      \n",
            "=================================================================\n",
            "Total params: 2,048\n",
            "Trainable params: 0\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 44032)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMf5MQBqvVOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5684224e-b08c-4ce0-d2bc-236cc5f4ba50"
      },
      "source": [
        "# COUGHVID DATASET\n",
        "!wget https://zenodo.org/record/4048312/files/public_dataset.zip?download=1\n",
        "!mv public_dataset.zip?download=1 public_dataset.zip\n",
        "!unzip -q public_dataset.zip\n",
        "!mkdir 'custom_dataset'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-19 02:50:00--  https://zenodo.org/record/4048312/files/public_dataset.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 951442487 (907M) [application/octet-stream]\n",
            "Saving to: ‘public_dataset.zip?download=1’\n",
            "\n",
            "public_dataset.zip? 100%[===================>] 907.37M  14.6MB/s    in 64s     \n",
            "\n",
            "2021-03-19 02:51:06 (14.2 MB/s) - ‘public_dataset.zip?download=1’ saved [951442487/951442487]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNVJb2NevnIa"
      },
      "source": [
        "coughvid  = '/content/public_dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1cyWbuBvpGO"
      },
      "source": [
        "VidData   = pd.read_csv(os.path.join(coughvid,'metadata_compiled.csv'),header=0)\n",
        "# VidData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdcUid9HvtZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7a9375-c56a-483c-bb77-6ebb54db21e5"
      },
      "source": [
        "labels = VidData['status'].unique()\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['healthy' nan 'COVID-19' 'symptomatic']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9a6RLjxvvfU"
      },
      "source": [
        "VidData   = VidData.loc[VidData['cough_detected'] >= 0.9][['uuid','fever_muscle_pain','respiratory_condition','status']]\n",
        "VidData.dropna(inplace=True)\n",
        "\n",
        "extradata = VidData.loc[VidData['status']=='COVID-19']\n",
        "notradata = VidData.loc[VidData['status']!='COVID-19'][0:1000]\n",
        "\n",
        "TotData   = pd.concat([extradata,notradata],ignore_index= True)\n",
        "TotData['DIR'] = coughvid + TotData['uuid'] + '.webm'\n",
        "TotData['DataSet'] = 'coughvid'\n",
        "TotData['fever_muscle_pain']    = TotData['fever_muscle_pain'].apply(int)\n",
        "TotData['respiratory_condition']= TotData['respiratory_condition'].apply(int)\n",
        "TotData   = pd.concat([TotData.rename(columns={'uuid':'ID','status':'STATUS','fever_muscle_pain':'Fever/MP','respiratory_condition':'ORC'})])\n",
        "TotData   = TotData.sample(frac=1).reset_index(drop=True)\n",
        "# TotData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSb0W-oxv4yO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dc420c-d509-43d7-b45a-9022eb1bb64a"
      },
      "source": [
        "labels = TotData['STATUS'].unique()\n",
        "print(labels, len(TotData))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['healthy' 'COVID-19' 'symptomatic'] 1441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oLQZNMt3rB_"
      },
      "source": [
        "# copy background noise to our new wav_dataset folder which wil contain separate directories for each class\n",
        "!mkdir /content/wav_dataset\n",
        "!cp \"/tmp/speech_commands_v0.02/_background_noise_\" -r /content/wav_dataset/_background_noise_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpppWAFS4ij8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# copy over subset of wav data that amil prepared\n",
        "!cp '/content/drive/MyDrive/coughvid_data2.zip' -r /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXu2wwp47tm"
      },
      "source": [
        "# unzip coughvid_data2 into /content/coughvid_data2\n",
        "!unzip -q coughvid_data2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iws3kuSK7fkK"
      },
      "source": [
        "# create class directories under wav_dataset for healhty and covid-19\n",
        "!mkdir '/content/wav_dataset/healthy'\n",
        "!mkdir '/content/wav_dataset/COVID-19'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8sVt9E77foH"
      },
      "source": [
        "healthy_data = []\n",
        "covid_19_data = []  # df[df['B']==3]['A']\n",
        "healthy_data.append(TotData[TotData['STATUS'] == \"healthy\"]['DIR'].values)\n",
        "covid_19_data.append(TotData[TotData['STATUS'] == \"COVID-19\"]['DIR'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTj_ClyD7ui6"
      },
      "source": [
        "# print(healthy_data) #837 healthy, 441 covid\n",
        "[healthy_data] = healthy_data\n",
        "healthy_data = healthy_data.tolist()\n",
        "print(healthy_data, type(healthy_data), len(healthy_data))\n",
        "\n",
        "[covid_19_data] = covid_19_data\n",
        "covid_19_data = covid_19_data.tolist()\n",
        "print(covid_19_data, type(covid_19_data), len(covid_19_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yRA7rnQ70hC"
      },
      "source": [
        "# clean file names in these lists\n",
        "\n",
        "healthy_data_2 = []\n",
        "covid_19_data_2 = []\n",
        "\n",
        "for i in healthy_data:\n",
        "  path = i.replace(\"/content/public_dataset/\", \"\")\n",
        "  healthy_data_2.append(path)\n",
        "for i in covid_19_data:\n",
        "  path = i.replace(\"/content/public_dataset/\", \"\")\n",
        "  covid_19_data_2.append(path)\n",
        "\n",
        "print(healthy_data_2)\n",
        "print(covid_19_data_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8cUAst078vU"
      },
      "source": [
        "import shutil, os\n",
        "\n",
        "# copy corresponding files into separate healthy or covid-19 directories under /content/\n",
        "for f in healthy_data_2:\n",
        "  name = os.path.splitext(f)[0] #get file name without extension\n",
        "  if name+\".wav\" in os.listdir('/content/coughvid_data2'):\n",
        "    # copy to /content/wav_dataset/healthy\n",
        "    shutil.copy(os.path.join('/content/coughvid_data2/', name + \".wav\"), '/content/wav_dataset/healthy')\n",
        "\n",
        "for f in covid_19_data_2:\n",
        "  name = os.path.splitext(f)[0]\n",
        "  if name+\".wav\" in os.listdir('/content/coughvid_data2'):\n",
        "    shutil.copy(os.path.join('/content/coughvid_data2/', name + \".wav\"), '/content/wav_dataset/COVID-19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXm66fju6cf"
      },
      "source": [
        "# Create some constants to be used later.\n",
        "\n",
        "# Target sampling rate. It is required by the audio preprocessing model.\n",
        "TARGET_SAMPLE_RATE = 44100\n",
        "# The specific audio tensor length expected by the preprocessing model.\n",
        "EXPECTED_WAVEFORM_LEN = preproc_model.input_shape[-1]\n",
        "\n",
        "# Where the Speech Commands v0.02 dataset has been downloaded.\n",
        "DATA_ROOT = \"/content/wav_dataset\" # \"/tmp/speech_commands_v0.02\"\n",
        "\n",
        "WORDS = (\"_background_noise_snippets_\", \"healthy\", \"COVID-19\") # (\"_background_noise_snippets_\", \"no\", \"yes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALkmssfqu8FS"
      },
      "source": [
        "# Unlike word examples, the noise samples in the Speech Commands v0.02 dataset\n",
        "# are not divided into 1-second snippets. Instead, they are stored as longer\n",
        "# recordings. Therefore we need to cut them up in to 1-second snippet .wav\n",
        "# files.\n",
        "\n",
        "noise_wav_paths = glob.glob(os.path.join(DATA_ROOT, \"_background_noise_\", \"*.wav\"))\n",
        "snippets_dir = os.path.join(DATA_ROOT, \"_background_noise_snippets_\")\n",
        "os.makedirs(snippets_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_snippets(wav_path, snippet_duration_sec=1.0):\n",
        "  basename = os.path.basename(os.path.splitext(wav_path)[0])\n",
        "  sample_rate, xs = wavfile.read(wav_path)\n",
        "  assert xs.dtype == np.int16\n",
        "  n_samples_per_snippet = int(snippet_duration_sec * sample_rate)\n",
        "  i = 0\n",
        "  while i + n_samples_per_snippet < len(xs):\n",
        "    snippet_wav_path = os.path.join(snippets_dir, \"%s_%.5d.wav\" % (basename, i))\n",
        "    snippet = xs[i : i + n_samples_per_snippet].astype(np.int16)\n",
        "    wavfile.write(snippet_wav_path, sample_rate, snippet)\n",
        "    i += n_samples_per_snippet\n",
        "\n",
        "for noise_wav_path in noise_wav_paths:\n",
        "  print(\"Extracting snippets from %s...\" % noise_wav_path)\n",
        "  extract_snippets(noise_wav_path, snippet_duration_sec=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sid2i6Ugu-Du"
      },
      "source": [
        "\n",
        "def resample_wavs(dir_path, target_sample_rate=44100):\n",
        "  \"\"\"Resample the .wav files in an input directory to given sampling rate.\n",
        "  \n",
        "  The resampled waveforms are written to .wav files in the same directory with\n",
        "  file names that ends in \"_44100hz.wav\".\n",
        "\n",
        "  44100 Hz is the sample rate required by the preprocessing model. It is also\n",
        "  the most widely supported sample rate among web browsers and mobile devices.\n",
        "  For example, see:\n",
        "  https://developer.mozilla.org/en-US/docs/Web/API/AudioContextOptions/sampleRate\n",
        "  https://developer.android.com/ndk/guides/audio/sampling-audio\n",
        "\n",
        "  Args:\n",
        "    dir_path: Path to a directory that contains .wav files.\n",
        "    target_sapmle_rate: Target sampling rate in Hz.\n",
        "  \"\"\"\n",
        "  wav_paths = glob.glob(os.path.join(dir_path, \"*.wav\"))\n",
        "  resampled_suffix = \"_%shz.wav\" % target_sample_rate\n",
        "  for i, wav_path in tqdm.tqdm(enumerate(wav_paths)):\n",
        "    if wav_path.endswith(resampled_suffix):\n",
        "      continue\n",
        "    sample_rate, xs = wavfile.read(wav_path)\n",
        "    xs = xs.astype(np.float32)\n",
        "    xs = librosa.resample(xs, sample_rate, TARGET_SAMPLE_RATE).astype(np.int16)\n",
        "    resampled_path = os.path.splitext(wav_path)[0] + resampled_suffix\n",
        "    wavfile.write(resampled_path, target_sample_rate, xs)\n",
        "\n",
        "\n",
        "for word in WORDS:\n",
        "  word_dir = os.path.join(DATA_ROOT, word)\n",
        "  assert os.path.isdir(word_dir)\n",
        "  resample_wavs(word_dir, target_sample_rate=TARGET_SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WMjuyfwvAHV"
      },
      "source": [
        "@tf.function\n",
        "def read_wav(filepath):\n",
        "  file_contents = tf.io.read_file(filepath)\n",
        "  return tf.expand_dims(tf.squeeze(tf.audio.decode_wav(\n",
        "      file_contents, \n",
        "      desired_channels=-1,\n",
        "      desired_samples=TARGET_SAMPLE_RATE).audio, axis=-1), 0)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def filter_by_waveform_length(waveform, label):\n",
        "  return tf.size(waveform) > EXPECTED_WAVEFORM_LEN\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def crop_and_convert_to_spectrogram(waveform, label):\n",
        "  cropped = tf.slice(waveform, begin=[0, 0], size=[1, EXPECTED_WAVEFORM_LEN])\n",
        "  return tf.squeeze(preproc_model(cropped), axis=0), label\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def spectrogram_elements_finite(spectrogram, label):\n",
        "  return tf.math.reduce_all(tf.math.is_finite(spectrogram))\n",
        "\n",
        "\n",
        "def get_dataset(input_wav_paths, labels):\n",
        "  \"\"\"Get a tf.data.Dataset given input .wav files and their labels.\n",
        "\n",
        "  The returned dataset emits 2-tuples of `(spectrogram, label)`, wherein\n",
        "  - `spectrogram` is a tensor of dtype tf.float32 and shape [43, 232, 1].\n",
        "    It is z-normalized (i.e., have a mean of ~0.0 and variance of ~1.0).\n",
        "  - `label` is a tensor of dtype tf.int32 and shape [] (scalar).\n",
        "  \n",
        "  Args:\n",
        "    input_wav_paths: Input audio .wav file paths as a list of string.\n",
        "    labels: integer labels (class indices) of the input .wav files. Must have\n",
        "      the same lengh as `input_wav_paths`.\n",
        "\n",
        "  Returns:\n",
        "    A tf.data.Dataset object as described above.\n",
        "  \"\"\"\n",
        "  ds = tf.data.Dataset.from_tensor_slices(input_wav_paths)\n",
        "  # Read audio waveform from the .wav files.\n",
        "  ds = ds.map(read_wav)\n",
        "  ds = tf.data.Dataset.zip((ds, tf.data.Dataset.from_tensor_slices(labels)))\n",
        "  # Keep only the waveforms longer than `EXPECTED_WAVEFORM_LEN`.\n",
        "  ds = ds.filter(filter_by_waveform_length)\n",
        "  # Crop the waveforms to `EXPECTED_WAVEFORM_LEN` and convert them to\n",
        "  # spectrograms using the preprocessing layer.\n",
        "  ds = ds.map(crop_and_convert_to_spectrogram)\n",
        "  # Discard examples that contain infinite or NaN elements.\n",
        "  ds = ds.filter(spectrogram_elements_finite)\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouJN4f7VvCbg"
      },
      "source": [
        "input_wav_paths_and_labels = []\n",
        "for i, word in enumerate(WORDS):\n",
        "  wav_paths = glob.glob(os.path.join(DATA_ROOT, word, \"*_%shz.wav\" % TARGET_SAMPLE_RATE))\n",
        "  print(\"Found %d examples for class %s\" % (len(wav_paths), word))\n",
        "  labels = [i] * len(wav_paths)\n",
        "  input_wav_paths_and_labels.extend(zip(wav_paths, labels))\n",
        "random.shuffle(input_wav_paths_and_labels)\n",
        "\n",
        "print(\"input_wav_paths_and_labels: \", input_wav_paths_and_labels)\n",
        "  \n",
        "input_wav_paths, labels = ([t[0] for t in input_wav_paths_and_labels],\n",
        "                           [t[1] for t in input_wav_paths_and_labels])\n",
        "dataset = get_dataset(input_wav_paths, labels)\n",
        "\n",
        "# Show some example spectrograms for inspection.\n",
        "fig = plt.figure(figsize=(40, 100))\n",
        "dataset_iter = iter(dataset)\n",
        "num_spectrograms_to_show = 10\n",
        "for i in range(num_spectrograms_to_show):\n",
        "  ax = fig.add_subplot(1, num_spectrograms_to_show, i + 1)\n",
        "  # print(dataset_iter)\n",
        "  spectrogram, label = next(dataset_iter)\n",
        "  spectrogram = spectrogram.numpy()\n",
        "  label = label.numpy()\n",
        "  plt.imshow(np.flipud(np.squeeze(spectrogram, -1).T), aspect=0.2)\n",
        "  ax.set_title(\"Example of \\\"%s\\\"\" % WORDS[label])\n",
        "  ax.set_xlabel(\"Time frame #\")\n",
        "  if i == 0:\n",
        "    ax.set_ylabel(\"Frequency bin #\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahbLVtE3vENd"
      },
      "source": [
        "# The amount of data we have is relatively small. It fits into typical host RAM\n",
        "# or GPU memory. For better training performance, we preload the data and\n",
        "# put it into numpy arrays:\n",
        "# - xs: The audio features (normalized spectrograms).\n",
        "# - ys: The labels (class indices).\n",
        "print(\n",
        "    \"Loading dataset and converting data to numpy arrays. \"\n",
        "    \"This may take a few minutes...\")\n",
        "xs = []\n",
        "ys = []\n",
        "print(EXPECTED_WAVEFORM_LEN)\n",
        "iterator = dataset.__iter__()\n",
        "for i in range(len(labels)):\n",
        "  # next_element = iterator.get_next()\n",
        "  try: \n",
        "    next_element = iterator.get_next_as_optional()\n",
        "    xs.append(next_element.get_value()[0].numpy())\n",
        "    ys.append(next_element.get_value()[1].numpy())\n",
        "  except:\n",
        "    continue\n",
        "    # print(next_element) \n",
        "  #   print(next_element.get_value()[0].numpy().shape)\n",
        "  #   print(next_element.get_value()[1].numpy().shape)\n",
        "\n",
        "  # print(next_element)\n",
        "  # print(next_element.has_value())\n",
        "\n",
        "xs = np.array(xs)\n",
        "ys = np.array(ys)\n",
        "# next_element = iterator.get_next()\n",
        "# xs_and_ys = list(dataset.as_numpy_iterator())\n",
        "# xs = np.stack([item[0] for item in xs_and_ys])\n",
        "# ys = np.stack([item[1] for item in xs_and_ys])\n",
        "\n",
        "# # MY ATTEMPT TO CONVERT DATA TO NP ARRAYS\n",
        "# # xs should be array of spectrograms, each of size (43, 232, 1)\n",
        "# # ys should be array of labels\n",
        "# # sample_rate, xs = wavfile.read(wav_path)\n",
        "# xs = []\n",
        "# for i in input_wav_paths:\n",
        "#   _, x = wavfile.read(i)\n",
        "# # change x to spectrogram\n",
        "#   xs.append(x)\n",
        "\n",
        "# ys = []\n",
        "# for y in labels:\n",
        "#   ys.append(y)\n",
        "\n",
        "# xs = np.array(xs)\n",
        "# ys = np.array(ys).astype('float32')\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pujlZGBz60P"
      },
      "source": [
        "dataset_shuffle = dataset.shuffle(int(0.3*533), reshuffle_each_iteration=True)\n",
        "xs_eval = []\n",
        "ys_eval = []\n",
        "iterator = dataset_shuffle.__iter__()\n",
        "for i in range(int(0.3*533)):\n",
        "  # next_element = iterator.get_next()\n",
        "  try: \n",
        "    next_element = iterator.get_next_as_optional()\n",
        "    xs_eval.append(next_element.get_value()[0].numpy())\n",
        "    ys_eval.append(next_element.get_value()[1].numpy())\n",
        "  except:\n",
        "    continue\n",
        "xs_eval = np.array(xs_eval)\n",
        "ys_eval = np.array(ys_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud_2McnivF6g"
      },
      "source": [
        "\n",
        "tfjs_model_json_path = '/tmp/tfjs-sc-model/model.json'\n",
        "\n",
        "# Load the Speech Commands model. Weights are loaded along with the topology,\n",
        "# since we train the model from scratch. Instead, we will perform transfer\n",
        "# learning based on the model.\n",
        "orig_model = tfjs.converters.load_keras_model(tfjs_model_json_path, load_weights=True)\n",
        "\n",
        "# Remove the top Dense layer and add a new Dense layer of which the output\n",
        "# size fits the number of sound classes we care about.\n",
        "model_coughvid = tf.keras.Sequential(name=\"TransferLearnedModel\")\n",
        "for layer in orig_model.layers[:-1]:\n",
        "  model_coughvid.add(layer)\n",
        "model_coughvid.add(tf.keras.layers.Dense(units=len(WORDS), activation=\"softmax\"))\n",
        "\n",
        "# Freeze all but the last layer of the model. The last layer will be fine-tuned\n",
        "# during transfer learning.\n",
        "for layer in model_coughvid.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "model_coughvid.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "model_coughvid.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwgMdKbt9tAq"
      },
      "source": [
        "# THESE ARE INPUTS/OUTPUTS THAT MODEL_COUGHVID EXPECTS: \n",
        "[print(i.shape, i.dtype) for i in model_coughvid.inputs]\n",
        "[print(o.shape, o.dtype) for o in model_coughvid.outputs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzlT7HZwvHnK"
      },
      "source": [
        "# Train the model.\n",
        "! pip install --upgrade wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(project=\"virufy_data\", entity=\"virufy-team\", name = \"speech_commands\")\n",
        "\n",
        "model_coughvid.fit(xs, ys, batch_size=256, validation_split=0.3, shuffle=True, epochs=50, callbacks=[WandbCallback()])\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXmrXg0iy04y"
      },
      "source": [
        "score = model_coughvid.evaluate(xs_eval, ys_eval)\n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asq_0-cJvKUR"
      },
      "source": [
        "# Convert the model to TensorFlow.js Layers model format.\n",
        "\n",
        "tfjs_model_dir = \"/tmp/tfjs-model-coughvid\"\n",
        "tfjs.converters.save_keras_model(model_coughvid, tfjs_model_dir)\n",
        "\n",
        "# Create the metadata.json file.\n",
        "metadata = {\"words\": [\"_background_noise_\"], \"frameSize\": model_coughvid.input_shape[-2]}\n",
        "metadata[\"words\"] += WORDS[1:]\n",
        "with open(os.path.join(tfjs_model_dir, \"metadata.json\"), \"w\") as f:\n",
        "  json.dump(metadata, f)\n",
        "\n",
        "!ls -lh /tmp/tfjs-model-coughvid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2d0qXR4gxu"
      },
      "source": [
        "# from keras.initializers import glorot_uniform\n",
        "# #Reading the model from JSON file\n",
        "# with open('/tmp/tfjs-model-coughvid/model.json', 'r') as json_file:\n",
        "#     json_savedModel= json_file.read()\n",
        "# #load the model architecture \n",
        "# model_j = tf.keras.models.model_from_json(json_savedModel)\n",
        "# model_j.summary()\n",
        "\n",
        "\n",
        "\n",
        "# from tensorflow import keras\n",
        "# # got weights and model from model_coughvid\n",
        "# weights = model_coughvid.get_weights()\n",
        "# config = model_coughvid.to_json()\n",
        "# loaded_model = tf.keras.models.model_from_json(config)\n",
        "\n",
        "# # load weights to model\n",
        "# loaded_model.set_weights(weights)\n",
        "\n",
        "# loaded_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "# score = loaded_model.evaluate(xs_eval, ys_eval)\n",
        "# print('Test loss:', score[0]) \n",
        "# print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnw6oHrkvM1A"
      },
      "source": [
        "\n",
        "# Convert the model to TFLite. \n",
        "\n",
        "# We need to combine the preprocessing model and the newly trained 3-class model\n",
        "# so that the resultant model will be able to preform STFT and spectrogram \n",
        "# calculation on mobile devices (i.e., without web browser's WebAudio).\n",
        "\n",
        "combined_model = tf.keras.Sequential(name='CombinedModel')\n",
        "combined_model.add(preproc_model)\n",
        "combined_model.add(model_coughvid)\n",
        "combined_model.build([None, EXPECTED_WAVEFORM_LEN])\n",
        "combined_model.summary()\n",
        "\n",
        "# tfjson_output_path = '/tmp/tfjs-sc-model/combined_model.json'\n",
        "# tfjs.converters.save_keras_model(combined_model, tfjson_output_path)\n",
        "# converter = combined_model.to_json()\n",
        "# loaded_model = tf.keras.models.model_from_json(config)\n",
        "\n",
        "# with open(tfjson_output_path, 'wb') as f:\n",
        "#     f.write(converter.convert())\n",
        "# print(\"Saved tfjson file at: %s\" % tflite_output_path)\n",
        "\n",
        "tflite_output_path = '/tmp/tfjs-sc-model/combined_model.tflite'\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(combined_model)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "with open(tflite_output_path, 'wb') as f:\n",
        "    f.write(converter.convert())\n",
        "print(\"Saved tflite file at: %s\" % tflite_output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwA56hFVl_gG"
      },
      "source": [
        "# ! pip install -q tf-nightly\n",
        "# ! pip install -q tensorflow-model-optimization\n",
        "\n",
        "# import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# # q_aware stands for for quantization aware.\n",
        "# q_aware_model = quantize_model(model_coughvid)\n",
        "\n",
        "# # `quantize_model` requires a recompile.\n",
        "# q_aware_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "# q_aware_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1SXVOSGmnSS"
      },
      "source": [
        "# wandb.init(project=\"virufy_data\", entity=\"virufy-team\", name = \"speech_commands_quantized\")\n",
        "\n",
        "# q_aware_model.fit(xs, ys, batch_size=256, validation_split=0.3, shuffle=True, epochs=50, callbacks=[WandbCallback()])\n",
        "\n",
        "# wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl-kZl7DoHsV"
      },
      "source": [
        "# # Convert the model to TensorFlow.js Layers model format.\n",
        "\n",
        "# tfjs_model_dir = \"/tmp/tfjs-model-coughvid-quant\"\n",
        "# tfjs.converters.save_keras_model(q_aware_model, tfjs_model_dir)\n",
        "\n",
        "# # Create the metadata.json file.\n",
        "# metadata = {\"words\": [\"_background_noise_\"], \"frameSize\": q_aware_model.input_shape[-2]}\n",
        "# metadata[\"words\"] += WORDS[1:]\n",
        "# with open(os.path.join(tfjs_model_dir, \"metadata.json\"), \"w\") as f:\n",
        "#   json.dump(metadata, f)\n",
        "\n",
        "# !ls -lh /tmp/tfjs-model-coughvid-quant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4-O-bvRq2tH"
      },
      "source": [
        "# quant_file = \"/tmp/tfjs-model-coughvid-quant/model.json\"\n",
        "# # with open(quant_file, 'wb') as f:\n",
        "# #   f.write(tfjs.converters.save_keras_model(q_aware_model, \"/tmp/tfjs-model-coughvid-quant\"))\n",
        "\n",
        "# float_file = \"/tmp/tfjs-model-coughvid/model.json\"\n",
        "# # with open(float_file, 'wb') as f:\n",
        "# #   f.write(tfjs.converters.save_keras_model(model_coughvid, \"/tmp/tfjs-model-coughvid\"))\n",
        "\n",
        "# print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "# print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}